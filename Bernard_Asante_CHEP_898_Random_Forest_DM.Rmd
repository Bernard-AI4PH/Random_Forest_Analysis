---
title: "Random Forest"
author: "Bernard Asante"
date: "2024-09-23"
output:
      html_document:
        toc: true
        toc_float: true 
        toc_depth: 3
---

# **Loading libraries**

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(sjPlot)
library(C50)
library(finalfit)
library(knitr)
library(gtsummary)
library(mlbench)
library(vip)
library(rsample)
library(rpart.plot)
library(tune)
library(recipes)
library(yardstick)
library(parsnip)
library(glmnet)
library(themis)
library(microbenchmark)
library(pROC)
```


# **Reading in data**

```{r}
data <- read_csv("mice_all_imp.csv")  


data <- data %>% mutate_at(3, factor)
data <- data %>% mutate_at(5:6, factor)
data <- data %>% mutate_at(8:12, factor)
data <- data %>% mutate_at(15:81, factor)
data <- data %>% mutate_at(83:93, factor)

data$ID <- NULL
data$ADM_STUDY_ID <- NULL
```


# **Exploratory Data Analysis on predictor**

Preparing diabetes variable. This will be used as an outcome variable.

From the data dictionary, DIS_DIAB_EVER DICTIONARY has 3 responses:
0 - "Never had diabetes"
1-  "Ever had diabetes"
2 - "Presumed-Never had diabetes"

```{r}
table(data$DIS_DIAB_EVER)
```
 From the can path dataset dictionary, 0 and 2 are almost the same. I want to recode them into binary where 0 and 2 will be recoded as "NO" and 1 recoded as "Yes"
 

## Recoding Predictor variable 

```{r}
data <- data %>%
	mutate(diabetes = case_when(
		DIS_DIAB_EVER == 0 ~ "No",
		DIS_DIAB_EVER == 1 ~ "Yes",
		DIS_DIAB_EVER == 2 ~ "No")) %>%
		mutate(diabetes = as.factor(diabetes))

data$DIS_DIAB_EVER <- NULL # Here I have to drop the main variable from the dataset

```

## Checking levels

```{r}
levels(data$diabetes)
```
I will have to factor the level to make "Yes" at level one and "NO" at second level


## Summary statistics of diabetes variable 

```{r}

data <- data %>% 
  mutate(diabetes = factor(diabetes,
                           levels = c("Yes", "No")))
data$diabetes %>% 
  summary()
```
I can see imbalance in the predictor variable. Let us do further analysis to understand it properly.
Here I will calculate the proportions of YEs and NO responses. 

## Calculating proportion of diabetes response

```{r}
prop_diabetes <- prop.table(table(data$diabetes)) * 100

prop_diabetes
```

I can see a high proportion of "No" responses(92.4%) compared to Yes with 7.56%. This imbalance is because I recoded 0 and 2 responses as "NO". I need to upsample the Yes responses to get a better model. 

Synthetic Minority Over-sampling Technique(SMOTE) will be implemented to increase the Yes responses. 

## Visualizing Diabetes response

```{r}
data %>% 
  count(diabetes) %>% 
  ggplot(aes(x = diabetes, y = n, fill = diabetes))+
  geom_col()  
```
This plot above shows the level of imbalance in the predictor variable


# **Random Forest Model**

## Splitting the dataset

```{r}
set.seed(10)


data_split <- initial_split(data, 
                            strata = diabetes, 
                            prop = 0.80)
```

The main dataset has to be split into 70% for train data and 30% for test data. Strata = "diabetes" esnures equal split of the predictor in test and training dataset 

## Training and Testing Data 

```{r}
train_data <- training(data_split)


test_data  <- testing(data_split)

```



## Detecting core 

```{r}
cores <- parallel::detectCores()
cores
```

Detecting the core of the machine because I will be doing parrallel compution on 7 cores to facilitate the process. 


## **Random forest model (Default hyperparameters)**

### Default RF model

This random model is build using default hyperparameters:
min_n = NULL
mtry = NULL


```{r}
rf_model_default <- rand_forest() %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")
```


### Building the recipe 

```{r}
rf_recipe_default <- recipe(diabetes ~ ., data = train_data) %>% 
  #step_dummy(all_nominal_predictors()) %>% 
  step_smotenc(diabetes, over_ratio = 0.90) %>%  # Upsampling the predictor variable to overcome the imbalance in the dataset. Here I used 15% of NO responses.It can be 90% or any reasonable percentage.  
  step_zv(all_predictors())
```

#### Visualizing train_data after SMOTE

```{r}
train_data_smote <- rf_recipe_default %>%
  prep() %>%
  bake(new_data = NULL)


train_data_smote %>%
  count(diabetes) %>%
  ggplot(aes(x = diabetes, y = n, fill = diabetes)) +
  geom_col() +
  ggtitle("Class Distribution After SMOTE")
```



### Building workflow 

```{r}
rf_workflow_default <- workflow() %>% 
  add_recipe(rf_recipe_default) %>% 
  add_model(rf_model_default)

rf_workflow_default
```

### Training the model 

```{r}
rf_workflow_default_fit <- rf_workflow_default %>% 
  fit(train_data)
```


### Predicting  Test Data 

#### Class Prediction 

```{r}
rf_predict_default <- predict(rf_workflow_default_fit, new_data = test_data, type = "class")

rf_predict_default %>% 
  head()
```
This is to predict the class of the test_data using the trained model. This is good to assess metrics such as accuracy, precision etc. 


#### Probability prediction

```{r}
rf_default_roc <- predict(rf_workflow_default_fit, new_data = test_data, type = "prob") %>% 
  bind_cols(test_data)

rf_default_roc %>% 
  select(.pred_No,.pred_Yes,diabetes) %>% 
  glimpse()
```

The probability predictions gives in probability(out of 1), how likely a prediction is to be NO or Yes. This predictions will be used for the ROC_CURVE later in the code




### combining test data and predictions

```{r}

rf_predict_default <- rf_predict_default %>% 
  cbind(test_data)   # This code is to add the predictions to the test_data. One can use bind_cols to achieve the same results 

rf_predict_default <- rf_predict_default %>% 
  select(.pred_class,diabetes) # Selecting only the variables I need for the model evaluation

rf_predict_default %>% 
  glimpse()
  
```

### Model Evaluation

#### Calculating metrics 

```{r}
c_matrix <- conf_mat(rf_predict_default, truth = diabetes, estimate = .pred_class)

c_matrix
```


```{r}
metrics <- metric_set(accuracy, sens, spec, precision)

metrics(rf_predict_default, truth = diabetes, estimate = .pred_class)

```


### Roc_Curve for default 

```{r}
roc_curves_default  <- rf_default_roc %>%
  mutate(diabetes = as.factor(diabetes)) %>% 
  roc_curve(truth = diabetes, .pred_Yes) %>% 
  autoplot()

roc_curves_default
```

This plot shows the trade-off between sensitivity (true positive rate) and 1 - specificity (false positive rate). The model has an AUC of about 0.80 which is very close to 1 indicating better predictive performance of the model to differentiate between true positives and false positives.


# **Correcting low sensitivity**

From the model above, I could see, sensitivity is 10% whereas specificity is close to 100%. 


##  Reeducing threshold

```{r}
rf_default_prob <- rf_default_roc %>% 
  select(.pred_No,.pred_Yes,diabetes) %>% 
  mutate(pred_class = ifelse(.pred_Yes > 0.23, "Yes", "No")) %>% 
  mutate(pred_class = as.factor(pred_class)) %>% 
  mutate(pred_class = factor(pred_class,
                             level = c("Yes", "No")))

rf_default_prob %>% 
  head()
```
## Evaluating model

```{r}
con_matrix <- conf_mat(rf_default_prob, truth = diabetes, estimate = pred_class)

con_matrix
```

```{r}
metrics <- metric_set(accuracy,sens,spec, precision)

metrics_default <- metrics(rf_default_prob, truth = diabetes,estimate =  pred_class)

metrics_default
```

The model is performing better(kinda). 


## **Random forest model (Hyperparameters tunning)**

### Building Tuned RF model

```{r}
rf_model <- rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>% 
              set_engine("ranger", num.threads = cores) %>% 
              set_mode("classification")
```

Here the hyperparameters are tuned to see the best value for each parameter that can give us the best predictions 


### Building the Recipe

```{r}
rf_recipe <- 
  recipe(diabetes ~ ., data = train_data) %>% 
  step_smotenc(diabetes,over_ratio = 0.50) %>% 
  step_zv(all_predictors()) 
```

### Building  a workflow

```{r}
rf_workflow <- 
        workflow() %>% 
        add_model(rf_model) %>% 
        add_recipe(rf_recipe)

rf_workflow
```

### Hyperparameter tunning 

```{r}
set.seed(100)

folds <- vfold_cv(train_data, v = 10) 

rf_grid <- grid_regular(
              mtry(range = c(1, 20)), 
              min_n(range = c(5, 50)),
              trees(range = c(1,100)),
              levels = 4
            )

rf_grid


rf_tune <- tune_grid(
                rf_workflow,
                resamples = folds,
                grid = rf_grid, 
                control = control_resamples(save_pred = TRUE, 
                                                  verbose = FALSE))

rf_tune
```

### Evaluating  Tunning metrics 
```{r}
tune_metrics <- rf_tune %>% 
  collect_metrics()

tune_metrics %>% 
  head() # Displaying the first 6 rows of the tune metrics 
```

#### Filtering only accuracy 

The best hyperparameter values will be based on levels of accuracy.

```{r}


tune_metrics <- tune_metrics %>% 
  filter(.metric == "accuracy") # Filtering accuracy to make it easy to compare hyperparameter values. 

tune_metrics %>% 
  head()
```
We are only interested in mean,min_n, trees and mtry

#### Selecting relevant variables  

```{r}
tune_long <- tune_metrics %>%
  select(mean, min_n, trees, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) 

tune_long %>%  
  head()
```


### Visualizing Tuned  hyperparameters 

```{r}
ggplot( tune_long, aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```


From the plot, the best tuned model will have mtry between one and 15, min_n of around 5  and trees of 100 


### Selecting best tuned model 

```{r}
rf_best <- 
  rf_tune %>% 
  select_best(metric = "accuracy")

rf_best  # we know the best model has 1 mtry and 50 min_n
```

The best model is preprocessor1_model51 becuase it has the best hyperparamters values as seen on the plot. mtry 13,trees = 100 adn  min_n of 50 

### Finalizing the best model 

```{r}
final_rf_model <- finalize_model(rf_model, rf_best)
```

### Creating final workflow
 
```{r}
rf_workflow_final <- workflow() %>% 
  add_model(final_rf_model) %>% 
  add_recipe(rf_recipe) 
```

### Training the final Model 

```{r}
rf_workflow_final_fit <- rf_workflow_final %>% 
  fit(train_data)

rf_workflow_final_fit
```

### Prediciting test data 

#### Class Prediction

```{r}
final_model_predict <- predict(rf_workflow_final_fit,new_data = test_data) %>% 
  bind_cols(test_data)

final_model_predict<- final_model_predict  %>%
  select(.pred_class, diabetes)

final_model_predict %>% 
  head()
```

#### Probabiltiy prediciton 

```{r}
rf_tuned_roc <- predict(rf_workflow_final_fit , new_data = test_data, type = "prob") %>% 
  bind_cols(test_data)
```



### Metrics on Tuned Models

#### Confusion Matrix


```{r}
conf_mat(final_model_predict, truth = diabetes,
         estimate = .pred_class)
```


#### Accuracy, Sens, Specs and Precision

```{r}
metrics <- metric_set(accuracy, sens, spec, precision)
metrics_tuned <- metrics(final_model_predict, truth = diabetes,
         estimate = .pred_class)
```
The RF tuned model has a very high accuracy of 92 % 

```{r}
metrics_tuned
```



### Variable Importance

```{r}
tree_prep <- prep(rf_recipe)

final_rf_model %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(diabetes ~ .,
    data = juice(tree_prep)) %>%
  vip(geom = "point")
```


### Roc Curve for tuned 

```{r}
roc_curves_tuned  <- rf_tuned_roc %>%
  yardstick::roc_curve(truth = diabetes, .pred_Yes) %>% 
  autoplot()

roc_curves_tuned
```

# **Correcting low sensitivity**

From the model above, I could see, sensitivity is 10% whereas specificity is close to 100%. 


##  Reeducing threshold

```{r}
rf_default_prob <- rf_tuned_roc %>% 
  select(.pred_No,.pred_Yes,diabetes) %>% 
  mutate(pred_class = ifelse(.pred_Yes > 0.23, "Yes", "No")) %>% 
  mutate(pred_class = as.factor(pred_class)) %>% 
  mutate(pred_class = factor(pred_class,
                             level = c("Yes", "No")))

rf_default_prob %>% 
  head()
```
## Evaluating model

```{r}
con_matrix <- conf_mat(rf_default_prob, truth = diabetes, estimate = pred_class)

con_matrix
```

```{r}
metric <- metric_set(accuracy,sens,spec, precision)

metrics_tuned <- metric(rf_default_prob, truth = diabetes,estimate =  pred_class)

metrics_tuned
```

## Default and Tuned Model Comparison 

### Reading metrics 

```{r}
metrics_default
metrics_tuned
```
### Labeling metrics 

```{r}
metrics_default <- metrics_default %>% 
  mutate(name = "Rf_Default")

metrics_default %>%  
  glimpse()
  
```
```{r}
metrics_tuned <- metrics_tuned %>% 
  mutate(name = "Rf_Tuned")

metrics_tuned %>%  
  glimpse()
```
### Binding metrics 

```{r}
metrics_eval <- bind_rows(metrics_default, metrics_tuned)

metrics_eval %>% 
  glimpse()
```

## Visualizing accuracy comparison 
```{r, fig.width=10, fig.height= 3}
ggplot(data = metrics_eval, aes(x = .metric, y = .estimate, fill = name))+
  geom_segment(aes(x = .metric, xend = .metric, yend = .estimate, y = 0))+
  geom_point(size = 4, colour = "#a13")+
  geom_text(aes(label = round(.estimate, 2)), hjust = -0.2, vjust = 0.5, color = "black")+
  coord_flip()
```
## Visualizing accuracy comparison 
```{r, fig.width=10, fig.height= 3}
ggplot(data = metrics_eval, aes(x = .metric, y = .estimate, fill = name ))+
  geom_col(position = "dodge")
```

## **Comparing RoC Curve**

```{r}
rf_default_roc <- rf_default_roc %>% 
  mutate(name = "Rf_Default")

rf_default_prob <- rf_default_prob %>% 
  mutate(name = "Rf_Tuned")

eval_roc <- bind_rows(rf_default_prob, rf_default_roc)


eval_roc <- eval_roc %>%  
  select(name,.pred_No,.pred_Yes,diabetes)


eval_roc %>% 
  glimpse()
```

```{r}
curve <- eval_roc %>%
  group_by(name) %>% 
  roc_curve(truth = diabetes, .pred_Yes) %>% 
  autoplot() +
  aes(color = name)

curve

```

