---
title: "Random Forest"
author: "Bernard Asante"
date: "2024-09-23"
output:
      html_document:
        toc: true
        toc_float: true 
        toc_depth: 3
---

# **Loading libraries**

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(sjPlot)
library(C50)
library(finalfit)
library(knitr)
library(gtsummary)
library(mlbench)
library(vip)
library(rsample)
library(rpart.plot)
library(tune)
library(recipes)
library(yardstick)
library(parsnip)
library(glmnet)
library(themis)
library(microbenchmark)
library(pROC)
```


# **Reading in data**

```{r}
data <- read_csv("mice_all_imp.csv")
```

## Converting variables to factor

```{r}
data <- data %>% mutate_at(3, factor)
data <- data %>% mutate_at(5:6, factor)
data <- data %>% mutate_at(8:12, factor)
data <- data %>% mutate_at(15:81, factor)
data <- data %>% mutate_at(83:93, factor)

data$ID <- NULL
data$ADM_STUDY_ID <- NULL
```


# **Exploratory Data Analysis on Outcome Variable**

Preparing diabetes variable. This will be used as an outcome variable.

From the data dictionary, DIS_DIAB_EVER DICTIONARY has 3 responses:
0 - "Never had diabetes"
1-  "Ever had diabetes"
2 - "Presumed-Never had diabetes"

```{r}
table(data$DIS_DIAB_EVER)
```
 From the can path dataset dictionary, 0 and 2 can be assumed to mean response for not having diabetes. I want to recode them into binary(dichotomous variable) where 0 and 2 will be recoded as "No" and 1 recoded as "Yes"
 

## Recoding Predictor variable 

```{r}
data <- data %>%
	mutate(diabetes = case_when(
		DIS_DIAB_EVER == 0 ~ "No",
		DIS_DIAB_EVER == 1 ~ "Yes",
		DIS_DIAB_EVER == 2 ~ "No")) %>%
		mutate(diabetes = as.factor(diabetes))
```


```{r}
data$DIS_DIAB_EVER <- NULL # I have to drop the main variable from the dataset

```

Both variables (DIS_DIAB_EVER and diabetes) convey essentially the same information.  Including both can result in redundancy and multicollinearity.  This redundancy can negatively impact model performance, interpretability, and variable significance measures.


## Checking levels

Here, i want to observe how the values of the outcome variable is arranged. From the dataset, which of  the responses("yes" and "No") is set as a preference. That is when used in a table or graph(barplot), which response will  come first. 

```{r}
levels(data$diabetes)
```
I will have to factor the level to make "Yes" at level one or reference in logistic models and "No" at second level


## Summary statistics of diabetes variable 

```{r}

data <- data %>% 
  mutate(diabetes = factor(diabetes,
                           levels = c("Yes", "No")))
data$diabetes %>% 
  summary()
```

I can see imbalance in the outcome variable. Let us do further analysis to understand it properly.
Here I will calculate the proportions of "Yes" and "No" responses. 

## Calculating proportion of diabetes response

```{r}
prop_diabetes <- prop.table(table(data$diabetes)) * 100

prop_diabetes
```

I can see a high proportion of "No" responses(92.4%) compared to Yes with 7.56%. This imbalance is because I recoded 0 and 2 responses as "NO". I need to upsample the Yes responses to get a better model. Synthetic Minority Over-sampling Technique(SMOTE) will be implemented to increase the Yes responses. 

## Visualizing Diabetes response

```{r}
data %>% 
  count(diabetes) %>% 
  ggplot(aes(x = diabetes, y = n, fill = diabetes))+
  geom_col()+
  labs(title = "Barplot of predictor variables", 
       y = "frequency")
```

This plot above shows the level of imbalance in the outcome variable


# **Random Forest Model**

## Splitting the dataset

```{r}
set.seed(10)


data_split <- initial_split(data, 
                            strata = diabetes, 
                            prop = 0.80)
```


The main dataset has to be split into 70% for train data and 30% for test data. Strata = "diabetes" ensures equal split of the outcome in test and training dataset 

## Training and Testing Data 

```{r}
train_data <- training(data_split)


test_data  <- testing(data_split)

```



## Detecting core 

```{r}
cores <- parallel::detectCores()
cores
```

Detecting the core of the machine because I will be doing parallel computation on 7 cores to facilitate the process. 


## **Random forest model (Default hyperparameters)**

### Default RF model

This random model is build using default hyperparameters:
min_n = NULL
mtry = NULL


```{r}
rf_model_default <- rand_forest() %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")
```


### Building the recipe 

```{r}
rf_recipe_default <- recipe(diabetes ~ ., data = train_data) %>% 
  #step_dummy(all_nominal_predictors()) %>% 
  step_smotenc(diabetes, over_ratio = 0.90) %>%  # Upsampling the predictor variable to overcome the imbalance in the dataset. Here I used 15% of NO responses.It can be 90% or any reasonable percentage.  
  step_zv(all_predictors())
```

#### Visualizing train_data after SMOTE

```{r}
train_data_smote <- rf_recipe_default %>%
  prep() %>%
  bake(new_data = NULL)


train_data_smote %>%
  count(diabetes) %>%
  ggplot(aes(x = diabetes, y = n, fill = diabetes)) +
  geom_col() +
  ggtitle("Class Distribution After SMOTE")
```



### Building workflow 

```{r}
rf_workflow_default <- workflow() %>% 
  add_recipe(rf_recipe_default) %>% 
  add_model(rf_model_default)

rf_workflow_default
```

### Training the model 

```{r}
rf_workflow_default_fit <- rf_workflow_default %>% 
  fit(train_data)
```


### Predicting  Test Data 

#### Class Prediction 

```{r}
rf_predict_default <- predict(rf_workflow_default_fit, new_data = test_data, type = "class")

rf_predict_default %>% 
  head()
```


This is to predict the class of the test_data using the trained model. This is good to assess metrics such as accuracy, precision etc. 


#### Probability prediction

```{r}
rf_default_roc <- predict(rf_workflow_default_fit, new_data = test_data, type = "prob") %>% 
  bind_cols(test_data)

rf_default_roc %>% 
  select(.pred_No,.pred_Yes,diabetes) %>% 
  glimpse()
```

The probability predictions gives in probability(out of 1), how likely a prediction is to be No or Yes. This predictions will be used for the ROC_CURVE later in the code


### combining test data and predictions

```{r}

rf_predict_default <- rf_predict_default %>% 
  cbind(test_data)   # This code is to add the predictions to the test_data. One can use bind_cols to achieve the same results 

rf_predict_default <- rf_predict_default %>% 
  select(.pred_class,diabetes) # Selecting only the variables I need for the model evaluation

rf_predict_default %>% 
  glimpse()
  
```

### Model Evaluation

#### Calculating metrics 

```{r}
c_matrix <- conf_mat(rf_predict_default, truth = diabetes, estimate = .pred_class)

c_matrix
```

The model predicts "No" well (high true negatives: 7470), but frequently misclassifies actual "Yes" cases, leading to many false negatives (557).

```{r}
metrics <- metric_set(accuracy, sens, spec, precision)

met_default <- metrics(rf_predict_default, truth = diabetes, estimate = .pred_class)

met_default

```

The model is highly accurate overall, with excellent specificity, but suffers from low sensitivity and poor precision predicting diabetes. The next approach is to reduce the threshold to predict positive to 


# **Correcting low sensitivity**

From the model above, we can see, sensitivity is 10% whereas specificity is close to 100%. 


##  Reeducing threshold

```{r}
rf_default_prob <- rf_default_roc %>% 
  select(.pred_No,.pred_Yes,diabetes) %>% 
  mutate(pred_class = ifelse(.pred_Yes > 0.23, "Yes", "No")) %>% 
  mutate(pred_class = as.factor(pred_class)) %>% 
  mutate(pred_class = factor(pred_class,
                             level = c("Yes", "No")))

rf_default_prob %>% 
  head()
```

This output shows predicted probabilities (.pred_No, .pred_Yes) and predicted class (pred_class). The model sometimes incorrectly predicts "Yes" despite moderate probabilities favoring "No".


## Evaluating model

```{r}
con_matrix <- conf_mat(rf_default_prob, truth = diabetes, estimate = pred_class)

con_matrix
```

```{r}
metrics <- metric_set(accuracy,sens,spec, precision)

metrics_default <- metrics(rf_default_prob, truth = diabetes,estimate =  pred_class)

metrics_default
```

The model has decent overall accuracy (75%), moderate sensitivity (60%) for detecting diabetes, good specificity (ability to detect non-diabetes), but very low precision. The model is performing better(kinda). 


### Roc_Curve for default 

```{r}
roc_curves_default  <- rf_default_roc %>%
  mutate(diabetes = as.factor(diabetes)) %>% 
  roc_curve(truth = diabetes, .pred_Yes) %>% 
  autoplot()

roc_curves_default
```

This plot shows the trade-off between sensitivity (true positive rate) and 1 - specificity (false positive rate). The model has an AUC of about 0.80 which is very close to 1 indicating better predictive performance of the model to differentiate between true positives and false positives.



## **Random forest model (Hyperparameters tunning)**

### Building Tuned RF model

```{r}
rf_model <- rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>% 
              set_engine("ranger", num.threads = cores) %>% 
              set_mode("classification")
```


Here the hyperparameters are tuned to see the best value for each parameter that can give us the best predictions 


### Building the Recipe

```{r}
rf_recipe <- 
  recipe(diabetes ~ ., data = train_data) %>% 
  step_smotenc(diabetes,over_ratio = 0.50) %>% 
  step_zv(all_predictors()) 
```

### Building  a workflow

```{r}
rf_workflow <- 
        workflow() %>% 
        add_model(rf_model) %>% 
        add_recipe(rf_recipe)

rf_workflow
```

### Hyperparameter tunning 

```{r}
set.seed(100)

folds <- vfold_cv(train_data, v = 10) 

rf_grid <- grid_regular(
              mtry(range = c(1, 20)), 
              min_n(range = c(5, 50)),
              trees(range = c(1,100)),
              levels = 4
            )

rf_grid


rf_tune <- tune_grid(
                rf_workflow,
                resamples = folds,
                grid = rf_grid, 
                control = control_resamples(save_pred = TRUE, 
                                                  verbose = FALSE))

rf_tune
```

### Evaluating  Tunning metrics 

```{r}
tune_metrics <- rf_tune %>% 
  collect_metrics()

tune_metrics %>% 
  head() # Displaying the first 6 rows of the tune metrics 
```

#### Filtering only accuracy 

The best hyperparameter values will be based on levels of accuracy.

```{r}


tune_metrics <- tune_metrics %>% 
  filter(.metric == "accuracy") # Filtering accuracy to make it easy to compare hyperparameter values. 

tune_metrics %>% 
  head()
```

We are only interested in mean,min_n, trees and mtry

#### Selecting relevant variables  

```{r}
tune_long <- tune_metrics %>%
  select(mean, min_n, trees, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) 

tune_long %>%  
  head()
```


### Visualizing Tuned  hyperparameters 

```{r}
ggplot( tune_long, aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```


From the plot, the best tuned model will have mtry between one and 15, min_n of around 5  and trees of 100 


### Selecting best tuned model 

```{r}
rf_best <- 
  rf_tune %>% 
  select_best(metric = "accuracy")

rf_best  # we know the best model has 1 mtry and 50 min_n
```

The best model is preprocessor1_model41 because it has the best hyperparameters values as seen on the plot. mtry 1,trees = 67  and  min_n of 35.

### Finalizing the best model 

```{r}
final_rf_model <- finalize_model(rf_model, rf_best)
```

### Creating final workflow
 
```{r}
rf_workflow_final <- workflow() %>% 
  add_model(final_rf_model) %>% 
  add_recipe(rf_recipe) 
```

### Training the final Model 

```{r}
rf_workflow_final_fit <- rf_workflow_final %>% 
  fit(train_data)

rf_workflow_final_fit
```

### Prediciting test data 

#### Class Prediction

```{r}
final_model_predict <- predict(rf_workflow_final_fit,new_data = test_data) %>% 
  bind_cols(test_data)

final_model_predict<- final_model_predict  %>%
  select(.pred_class, diabetes)

final_model_predict %>% 
  glimpse()
```

#### Probabiltiy prediciton 


```{r}
rf_tuned_roc <- predict(rf_workflow_final_fit , new_data = test_data, type = "prob") %>% 
  bind_cols(test_data)
```



### Metrics on Tuned Models

#### Confusion Matrix


```{r}
conf_mat(final_model_predict, truth = diabetes,
         estimate = .pred_class)
```

We can see the model is predicting less true positives resulting in more false positives where as there is higher predicition of true negatives and less false positive.

#### Accuracy, Sens, Specs and Precision

```{r}
metrics <- metric_set(accuracy, sens, spec, precision)
metrics_tune <- metrics(final_model_predict, truth = diabetes,
         estimate = .pred_class)
```

The RF tuned model has a very high accuracy of 92 % 

```{r}
metrics_tune
```

The model has very high accuracy and specificity, but extremely low sensitivity and poor precision, indicating it fails to detect positives.

### Variable Importance

```{r}
tree_prep <- prep(rf_recipe)

final_rf_model %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(diabetes ~ .,
    data = juice(tree_prep)) %>%
  vip(geom = "point")
```

The plot you above depicts Permutation Feature Importance, which indicates variables that had the largest influence on predicting diabetes in the random forest model.Higher significance scores indicate that the predictor has a considerable influence on the model's accuracy. The variables at the top of the graph (DIS_HBP_EVER, HS_GEN_HEALTH, and DIS_ENDO_HB_CHOL_EVER) are the most important for predicting diabetes.

### Roc Curve for tuned 

```{r}
roc_curves_tuned  <- rf_tuned_roc %>%
  yardstick::roc_curve(truth = diabetes, .pred_Yes) %>% 
  autoplot()

roc_curves_tuned
```

The ROC curve reveals that the model classifies considerably between diabetes and no diabetes, outperforming the diagonal line(random guess) but remaining limited. The curve's location suggests fair but poor prediction performance, implying that the model requires improvement to efficiently detect real diabetes patients with minimal errors.


# **Correcting low sensitivity**

```{r}
metrics_tune
```


From the metrics tune above, we could see, sensitivity is extremely low whereas specificity is close to 100%. This means, "Yes" prediction has a lower threshold compared to the default 0.5. The next step is to reduce the threshold


##  Reducing threshold

```{r}
rf_default_prob <- rf_tuned_roc %>% 
  select(.pred_No,.pred_Yes,diabetes) %>% 
  mutate(pred_class = ifelse(.pred_Yes > 0.30, "Yes", "No")) %>% 
  mutate(pred_class = as.factor(pred_class)) %>% 
  mutate(pred_class = factor(pred_class,
                             level = c("Yes", "No")))

rf_default_prob %>% 
  head()
```

The code above is to reduce the threshold for predicting Yes from 0.5 to 0.30 to see how it affects sensitivity and specificity

## Evaluating model

```{r}
con_matrix <- conf_mat(rf_default_prob, truth = diabetes, estimate = pred_class)

con_matrix
```

The model struggles to detect true positives (low sensitivity) but it is better than when we used threshold at 0.5

```{r}
metric <- metric_set(accuracy,sens,spec, precision)

metrics_tuned <- metric(rf_default_prob, truth = diabetes,estimate =  pred_class)

metrics_tuned
```

# **Model Comparison**

## Using default threshold(0.5)

### Labeling metrics 

```{r}
metric_default <- met_default %>% 
  mutate(name = "Rf_Default")

metric_default %>%  
  glimpse()
  
```
```{r}
metric_tuned <- metrics_tune %>% 
  mutate(name = "Rf_Tuned")

metric_tuned %>%  
  glimpse()
```

### Binding metrics 

```{r}
metric_eval <- bind_rows(metric_default, metric_tuned)

metric_eval %>% 
  glimpse()
```

## Visualizing metrics comparison 

```{r, fig.width=10, fig.height= 3}
ggplot(data = metric_eval, aes(x = .metric, y = .estimate, fill = name ))+
  geom_col(position = "dodge")+
  labs(title = "Barplot comparing tuned and default hyperparamter models",
       subtitle = "Threshold ~ 0.5")
```

Accuracy and specificity relatively improved after tuning comapred to default hyperameters however, tuning does not improve sensitivity and precision of the model. 



## Adjusted threshold

### Labeling metrics 

```{r}
metrics_default
metrics_tuned
```
### Labeling metrics 

```{r}
metrics_default <- metrics_default %>% 
  mutate(name = "Rf_Default")

metrics_default %>%  
  glimpse()
  
```
```{r}
metrics_tuned <- metrics_tuned %>% 
  mutate(name = "Rf_Tuned")

metrics_tuned %>%  
  glimpse()
```
### Binding metrics 

```{r}
metrics_eval <- bind_rows(metrics_default, metrics_tuned)

metrics_eval %>% 
  glimpse()
```

## Visualizing metrics comparison 

```{r, fig.width=10, fig.height= 3}
ggplot(data = metrics_eval, aes(x = .metric, y = .estimate, fill = name ))+
  geom_col(position = "dodge")+
  geom_col(position = "dodge")+
  labs(title = "Barplot comparing tuned and default hyperparamter models",
       subtitle = "Threshold ~ adjusted")
```

After reducing the threshold for prediciting Yes, the overal accuracy reduced but sensitivity and specificity achieved considerable increase compared to when default threshold of 0.5 was used.

## **Comparing RoC Curve**

```{r}
rf_default_roc <- rf_default_roc %>% 
  mutate(name = "Rf_Default")

rf_default_prob <- rf_default_prob %>% 
  mutate(name = "Rf_Tuned")

eval_roc <- bind_rows(rf_default_prob, rf_default_roc)


eval_roc <- eval_roc %>%  
  select(name,.pred_No,.pred_Yes,diabetes)


eval_roc %>% 
  glimpse()
```

```{r}
curve <- eval_roc %>%
  group_by(name) %>% 
  roc_curve(truth = diabetes, .pred_Yes) %>% 
  autoplot() +
  aes(color = name)

curve

```

From the ROC_CURVE, the area under the curve for the default is bigger compared to the tuned model. This means the default model had better overall prediction ability compared to the tuned model. 
