---
title: "Random Forest"
author: "Bernard Asante"
date: "2024-09-23"
output:
      html_document:
        toc: true
        toc_float: true 
        toc_depth: 3
---

# **Loading libraries**

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(sjPlot)
library(C50)
library(finalfit)
library(knitr)
library(gtsummary)
library(mlbench)
library(vip)
library(rsample)
library(rpart.plot)
library(tune)
library(recipes)
library(yardstick)
library(parsnip)
library(glmnet)
library(themis)
library(microbenchmark)
library(pROC)
```


# **Reading in data**

```{r}
data <- read_csv("mice_all_imp.csv")  


data <- data %>% mutate_at(3, factor)
data <- data %>% mutate_at(5:6, factor)
data <- data %>% mutate_at(8:12, factor)
data <- data %>% mutate_at(15:81, factor)
data <- data %>% mutate_at(83:93, factor)

data$ID <- NULL
data$ADM_STUDY_ID <- NULL
```

# **Exploratory Data Analysis on predictor**

Preparing gen_health variable. This will be used as an outocome variable

From the data dictionary, HS_GEN_HEALTH has 4 responses:
1 - "Poor"
2 - "fair"
3-  "good"
4 - "Very good"
5- "Excellent"
```{r}
table(data$	HS_GEN_HEALTH)
```
I want to recode the variables into 3 categories : "poor health", "good health" and "excellent health"   
From the can path dataset dictionary, 1 and 2 are almost the same. I want to recode them into "poor" category, recode 3 and 4 as "good" health and 5 as "excellent" health.
 

## Recoding Predictor variable 

```{r}
data <- data %>%
	mutate(gen_health = case_when(
		HS_GEN_HEALTH == 1 ~ 1,
		HS_GEN_HEALTH == 2 ~ 1,
		HS_GEN_HEALTH == 3 ~ 2,
		HS_GEN_HEALTH == 4 ~ 3,	
		HS_GEN_HEALTH == 5 ~ 3)) %>%
		mutate(gen_health = as.factor(gen_health))

table(data$HS_GEN_HEALTH, data$gen_health)

data$HS_GEN_HEALTH <- NULL

```


## Summary statistics of diabetes variable 

```{r}
data$gen_health %>% 
  summary()
```
I can see imbalance in the predictor variable. Let us do further analysis to understand it properly.
Here I will calculate the proportions of 1, 2 and 3. 

## Calculating proportion of diabetes response

```{r}
prop_gen_health <- prop.table(table(data$gen_health)) * 100

prop_gen_health
```

I can see a high proportion of "3" responses(58.7%) compared to 2 with 31.04% and 1 as 10.212%. This imbalance is because I recoded 1 and 2 responses as "poor" both having the least responses compared to 3,4 and 5. I need to upsample the 1 responses to get a better model. 

Synthetic Minority Over-sampling Technique(SMOTE) will be implemented to increase the "1" responses. 

## Visualizing Diabetes response

```{r}
data %>% 
  count(gen_health) %>% 
  ggplot(aes(x = gen_health, y = n, fill = gen_health))+
  geom_col()  
```
This plot above shows the level of imbalance in the predictor variable


## Simplifying the data

```{r}
data_small <- select(data, gen_health, SDC_AGE_CALC, SDC_EDU_LEVEL, PM_BMI_SR, WRK_FULL_TIME, SMK_CIG_EVER, SDC_INCOME, PA_TOTAL_SHORT, HS_ROUTINE_VISIT_EVER, PSE_ADULT_WRK_DURATION, DIS_RESP_SLEEP_APNEA_EVER, SDC_EDU_LEVEL_AGE, SDC_GENDER)
```



# **Random Forest Model**

## Splitting the dataset

```{r}
set.seed(10)


data_split <- initial_split(data, 
                            strata = gen_health, 
                            prop = 0.70)
```

The main dataset has to be split into 70% for train data and 30% for test data. Strata = "gen_health" ensures equal split of the predictor in test and training dataset 

## Training and Testing Data 

```{r}
train_data <- training(data_split)


test_data  <- testing(data_split)

```


## Detecting core 

```{r}
cores <- parallel::detectCores()
cores
```

Detecting the core of the machine because I will be doing parallel computing on 7 cores to facilitate the process. 


## **Random forest model (Default hyperparameters)**

### Default RF model

This random model is build using default hyperparameters:
min_n = NULL
mtry = NULL


```{r}
rf_model_default <- rand_forest() %>% 
  set_engine("ranger", num.threads = 8) %>% 
  set_mode("classification")
```


### Building the recipe 

```{r}
rf_recipe_default <- recipe(gen_health ~ ., data = train_data) %>% 
  step_smotenc(gen_health, over_ratio = 0.90) %>%  # Upsampling the minority class in the predictor variable to overcome the imbalance in the dataset. Here I used 50% of the majority responses.It can be 90% or any reasonable percentage.  
  step_zv(all_predictors())
```


```{r}
prepped_recipe <- prep(rf_recipe_default, training = train_data)

baked_data <- bake(prepped_recipe, new_data = train_data)


upsampled_distribution <- table(baked_data$gen_health) # This is to show the distribution

upsampled_distribution

data$gen_health %>% 
  summary()

upsample = data.frame(upsampled_distribution)
upsample
```


```{r}
ggplot(upsample,aes(x = Var1, y = Freq))+
  geom_col()


```



### Building workflow 

```{r}
rf_workflow_default <- workflow() %>% 
  add_recipe(rf_recipe_default) %>% 
  add_model(rf_model_default)

rf_workflow_default
```

### Training the model 

```{r}
rf_workflow_default_fit <- rf_workflow_default %>% 
  fit(train_data)
```


### Predicting  Test Data 

#### Class Prediction 

```{r}
rf_predict_default <- predict(rf_workflow_default_fit, new_data = test_data, type = "class")%>% 
  bind_cols(test_data)


```

This is to predict the class of the test_data using the trained model. This is good to assess metrics such as accuracy, precision etc. 


#### Probability prediction

```{r}
rf_default_roc <- predict(rf_workflow_default_fit, new_data = test_data, type = "prob") %>% 
  bind_cols(test_data)



rf_default_roc %>% 
  select(.pred_1,.pred_2,.pred_3, gen_health) %>% 
  head()
```
The probability predictions gives in probability(out of 1), how likely a prediction is to be 1,2  or 3. 



### combining test data and predictions

```{r}

rf_predict_default <- rf_predict_default %>% 
  select(.pred_class,gen_health)

rf_predict_default %>% 
  glimpse()
  
```


### Model Evaluation

#### Calculating metrics 

```{r}
default_accuracy <- accuracy(rf_predict_default, truth = gen_health, estimate = .pred_class)
default_sensitivity <- sens(rf_predict_default, truth = gen_health, estimate = .pred_class)
default_specificity <- spec(rf_predict_default, truth = gen_health, estimate = .pred_class)
default_precision <- precision(rf_predict_default, truth = gen_health, estimate = .pred_class)
default_f_score <- f_meas(rf_predict_default, truth = gen_health, estimate = .pred_class)
```

#### Combine metrics into a data frame

```{r}
default_metrics <- data.frame(
  accuracy_default = default_accuracy$.estimate,
  sensitivity_default = default_sensitivity$.estimate,
  specificity_default = default_specificity$.estimate,
  precision_default = default_precision$.estimate,
  f_score_default = default_f_score$.estimate
)

default_metrics %>% 
  head()
```

#### Changing shape of dataframe 

```{r}
default_metrics_long <- default_metrics %>% 
  pivot_longer(cols = accuracy_default:f_score_default,
               names_to = "metrics",
               values_to = "estimates")

default_metrics_long
```


## **Random forest model (Hyperparameters tunning)**

### Building Tuned RF model

```{r}
rf_model <- rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>% 
              set_engine("ranger", num.threads = cores) %>% 
              set_mode("classification")
```

Here the hyperparameters are tuned to see the best value for each parameter that can give us the best predictions 


### Building the Recipe

```{r}
rf_recipe <- 
  recipe(gen_health ~ ., data = train_data) %>% 
  step_smotenc(gen_health,over_ratio = 0.90) %>% 
  step_zv(all_predictors()) 
```

### Building  a workflow

```{r}
rf_workflow <- 
        workflow() %>% 
        add_model(rf_model) %>% 
        add_recipe(rf_recipe)

rf_workflow
```

### Hyperparameter tunning 

```{r}
set.seed(100)

folds <- vfold_cv(train_data, v = 10) 

rf_grid <- grid_regular(
              mtry(range = c(1, 20)), 
              min_n(range = c(5, 50)),
              trees(range = c(1,100)),
              levels = 4
            )

rf_grid


rf_tune <- tune_grid(
                rf_workflow,
                resamples = folds,
                grid = rf_grid, 
                control = control_resamples(save_pred = TRUE, 
                                                  verbose = FALSE))

rf_tune
```

### Evaluating  Tunning metrics 

```{r}
tune_metrics <- rf_tune %>% 
  collect_metrics()

tune_metrics %>% 
  head() # Displaying the first 6 rows of the tune metrics 
```

#### Filtering only accuracy 

The best hyperparameter values will be based on levels of accuracy.

```{r}


tune_metrics <- tune_metrics %>% 
  filter(.metric == "accuracy") # Filtering accuracy to make it easy to compare hyperparameter values. 

tune_metrics %>% 
  head()
```
We are only interested in mean and the hyperparameter variables; min_n, trees and mtry

#### Selecting relevant variables  

```{r}
tune_long <- tune_metrics %>%
  select(mean, min_n, trees, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) 

tune_long %>%  
  head()
```


### Visualizing Tuned  hyperparameters 

```{r}
ggplot( tune_long, aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```


From the plot, the best tuned model will have mtry of 7, min_n of around 5  and trees of 100 


### Selecting best tuned model 

```{r}
rf_best <- 
  rf_tune %>% 
  select_best(metric = "accuracy")

rf_best  # we know the best model has 1 mtry and 50 min_n
```

The best model is preprocessor1_model50 becuase it has the best hyperparamters values as seen on the plot. mtry 7,trees = 100 and  min_n of 5 

### Finalizing the best model 

```{r}
final_rf_model <- finalize_model(rf_model, rf_best)
```

### Creating final workflow
 
```{r}
rf_workflow_final <- workflow() %>% 
  add_model(final_rf_model) %>% 
  add_recipe(rf_recipe) 
```

### Training the final Model 

```{r}
rf_workflow_final_fit <- rf_workflow_final %>% 
  fit(train_data)

rf_workflow_final_fit
```

### Prediciting test data 

#### Class Prediction

```{r}
final_model_predict <- predict(rf_workflow_final_fit,new_data = test_data) %>% 
  bind_cols(test_data)

final_model_predict<- final_model_predict  %>%
  select(.pred_class, gen_health)

final_model_predict %>% 
  head()
```

#### Probabiltiy prediciton 

```{r}
rf_tuned_roc <- predict(rf_workflow_final_fit , new_data = test_data, type = "prob") %>% 
  bind_cols(test_data) %>% 
  select(.pred_1,.pred_2,.pred_3, gen_health)

rf_tuned_roc %>% 
  head()
```



### Metrics on Tuned Models

#### Confusion Matrix


```{r}
conf_mat(final_model_predict, truth = gen_health,
         estimate = .pred_class)
```


#### Accuracy


```{r}
accuracy_tuned <- accuracy(final_model_predict, truth = gen_health,
         estimate = .pred_class)

accuracy_tuned
```
The model has moderate accuracy (61.96%), meaning it predicts correctly most of the time, but it still makes mistakes.

#### Sensitivity

```{r}
sensitivity_tuned <- sens(final_model_predict, truth = gen_health,
         estimate = .pred_class)

sensitivity_tuned
```
Sensitivity (49.5%) is lower indicating that the model misses a lot of actual cases creating a lot of false negatives 


#### Specificity

```{r}
specificity_tuned <- spec(final_model_predict, truth = gen_health,
         estimate = .pred_class)

specificity_tuned
```
Sensitivity (49.5%) is lower than specificity (75.04%), indicating that the model misses a lot of actual cases but is better at avoiding false positives.

#### F1 Score

```{r}
f1_score_tuned <- f_meas(final_model_predict, truth = gen_health,
         estimate = .pred_class)

f1_score_tuned
```

The F1-score (50.44%) suggests the model is not optimally balanced in handling precision and sensitivity.

#### Precision

```{r}
precision_tuned  <- precision(final_model_predict, truth = gen_health,
         estimate = .pred_class) 

precision_tuned
```
Precision (52.89%) is not very high, meaning that many of the model's positive predictions might be incorrect.



#### Combine metrics into a data frame

```{r}
tuned_metrics <- data.frame(
  accuracy_tuned = accuracy_tuned$.estimate,
  sensitivity_tuned = sensitivity_tuned$.estimate,
  specificity_tuned = specificity_tuned$.estimate,
  precision_tuned= precision_tuned$.estimate,
  f_score_tuned = f1_score_tuned$.estimate
)

tuned_metrics %>% 
  head()
```



#### Changing shape of dataframe 

```{r}
tuned_metrics_long <- tuned_metrics %>% 
  pivot_longer(cols = accuracy_tuned:f_score_tuned,
               names_to = "metrics",
               values_to = "estimates")

tuned_metrics_long
```
The model has moderate accuracy (61.96%), meaning it predicts correctly most of the time, but it still makes mistakes.
Sensitivity (49.5%) is lower than specificity (75.04%), indicating that the model misses a lot of actual cases but is better at avoiding false positives.
Precision (52.89%) is not very high, meaning that many of the model's positive predictions might be incorrect.
The F1-score (50.44%) suggests the model is not optimally balanced in handling precision and sensitivity.

### Variable Importance

```{r}
tree_prep <- prep(rf_recipe)

final_rf_model %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(gen_health ~ .,
    data = juice(tree_prep)) %>%
  vip(geom = "point")
```

The top three features influencing general health classification are income (SDC_INCOME), education level (SDC_EDUCATION), and history of high blood pressure (DIS_HBP_EVER), highlighting socioeconomic and medical factors' impact on health outcomes.


## Default and Tuned Model Comparison 

```{r}
tuned_metrics_long

default_metrics_long
```

```{r}
metrics_combined <- bind_rows(tuned_metrics_long, default_metrics_long)

metrics_combined %>% 
  glimpse()
```

### Accuracy Comparison

```{r}
accuracy_compare <- metrics_combined %>% 
  filter(metrics %in% c("accuracy_tuned", "accuracy_default"))
```

#### Visualizing accuracy comparison 

```{r, fig.width=10, fig.height= 3}
ggplot(data = accuracy_compare, aes(x = metrics, y = estimates))+
  geom_segment(aes(x = metrics, xend = metrics, yend = estimates, y = 0))+
  geom_point(size = 4, colour = "#a13")+
  geom_text(aes(label = round(estimates, 2)), hjust = -0.2, vjust = 0.5, color = "black")+
  coord_flip()
```
### sensitivity Comparison

```{r}
sensitivity_compare <- metrics_combined %>% 
  filter(metrics %in% c("sensitivity_tuned", "sensitivity_default"))
```

#### Visualizing sensitivity comparison

```{r, fig.width=10, fig.height= 3}
ggplot(data = sensitivity_compare, aes(x = metrics, y = estimates))+
  geom_segment(aes(x = metrics, xend = metrics, yend = estimates, y = 0))+
  geom_point(size = 4, colour = "#a13")+
  geom_text(aes(label = round(estimates, 2)), hjust = -0.2, vjust = 0.5, color = "black")+
  coord_flip()
```


### Precision Comparison

```{r}
precision_compare <- metrics_combined %>% 
  filter(metrics %in% c("precision_tuned", "precision_default"))
```

#### Visualizing precision comparison

```{r, fig.width=10, fig.height= 3}
ggplot(data = precision_compare, aes(x = metrics, y = estimates))+
  geom_segment(aes(x = metrics, xend = metrics, yend = estimates, y = 0))+
  geom_point(size = 4, colour = "#a13")+
  geom_text(aes(label = round(estimates, 2)), hjust = -0.2, vjust = 0.5, color = "black")+
  coord_flip()
```

